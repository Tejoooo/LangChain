{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "USER_VECTOR_DB_PATH = \"user_vector_stores\"  # Base directory for all users' vector stores\n",
    "\n",
    "def get_user_vector_store(user_id, embeddings):\n",
    "    \"\"\"\n",
    "    Load the FAISS vector store for the given user. If it doesn't exist, create a new one.\n",
    "    \"\"\"\n",
    "    user_db_path = os.path.join(USER_VECTOR_DB_PATH, user_id)\n",
    "    \n",
    "    if os.path.exists(user_db_path):  # Load existing vector store\n",
    "        print(f\"Loading vector store for user: {user_id}\")\n",
    "        return FAISS.load_local(user_db_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    else:  # Create a new vector store for this user\n",
    "        print(f\"Creating new vector store for user: {user_id}\")\n",
    "        single_vector = embeddings.embed_query(\"this is some text data\")\n",
    "        index = faiss.IndexFlatL2(len(single_vector))\n",
    "        \n",
    "        return FAISS(\n",
    "            embedding_function=embeddings,\n",
    "            index=index,\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Archents1\\LangChain\\test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def process_and_store_user_data(user_id, source_file, embeddings):\n",
    "    \"\"\"\n",
    "    Convert the document, split it, and store embeddings in the user's vector database.\n",
    "    \"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(source_file)\n",
    "\n",
    "    markdown_text = result.document.export_to_markdown()\n",
    "    docs = [Document(page_content=markdown_text)]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=1000)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Load or create user-specific vector store\n",
    "    vector_store = get_user_vector_store(user_id, embeddings)\n",
    "\n",
    "    # Add documents to the userâ€™s vector store\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "\n",
    "    # Save the updated vector store\n",
    "    user_db_path = os.path.join(USER_VECTOR_DB_PATH, user_id)\n",
    "    vector_store.save_local(user_db_path)\n",
    "    print(f\"User {user_id} data stored successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2:1b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "model.invoke(\"hi\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_retriever(user_id, embeddings):\n",
    "    \"\"\"\n",
    "    Retrieve the appropriate user's data.\n",
    "    \"\"\"\n",
    "    vector_store = get_user_vector_store(user_id, embeddings)\n",
    "    \n",
    "    return vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\n",
    "        'k': 3, \n",
    "        'fetch_k': 100,\n",
    "        'lambda_mult': 1\n",
    "    })\n",
    "\n",
    "def user_rag_pipeline(user_id, question, model, embeddings):\n",
    "    \"\"\"\n",
    "    Process the query using the RAG pipeline for the given user.\n",
    "    \"\"\"\n",
    "    retriever = get_user_retriever(user_id, embeddings)\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain.invoke(question)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store for user: user_123\n",
      "User user_123 data stored successfully.\n",
      "Loading vector store for user: user_456\n",
      "User user_456 data stored successfully.\n",
      "Loading vector store for user: user_456\n",
      "User 2 Response: | Molecule Class          | Good Response   | Intermediate Response   | Poor Response                                                      | Evidence not  found                                                        |\n",
      "|-------------------------|-----------------|-------------------------|--------------------------------------------------------------------|----------------------------------------------------------------------------|\n",
      "| Sulfonylureas           | Metformin       | Gliclazides,  Glibenclamide,  Glimepiride,  Glipizide,  Gliquidone   | Glyburide  Chlorpropamide  Tolazamide  Tolbutamide                           |\n",
      "| GLP-1 receptor agonists | Liraglutide             | Exenatide  Liraglunatide Lixisenatide  Dulagutide  Albiglutide Semaglutide      | Glimepiride  Gliclazides,  Glibenclamide,  Glimepiride,  Gliquidone    |\n",
      "\n",
      "The provided context does not contain information about anti-hypertensive drugs. Therefore, I do not know the output structure for this question as it pertains to pharmacogenomics and drug response rather than a specific medical condition like hypertension.\n"
     ]
    }
   ],
   "source": [
    "# User 1: Processing and Storing Data\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text', base_url=\"http://localhost:11434\")\n",
    "\n",
    "user_1 = \"user_123\"\n",
    "pdf_path_1 = \"C:\\\\Users\\\\Archents1\\\\LangChain\\\\4.FAISS multiple users\\\\GenepoweRx Total New Sample Report 23-44-46.pdf\"\n",
    "process_and_store_user_data(user_1, pdf_path_1, embeddings)\n",
    "\n",
    "# User 2: Processing and Storing Data\n",
    "user_2 = \"user_456\"\n",
    "pdf_path_2 = \"C:\\\\Users\\\\Archents1\\\\LangChain\\\\4.FAISS multiple users\\\\Ghost Y-Genetic Report (1)-46-53-2-3.pdf\"\n",
    "process_and_store_user_data(user_2, pdf_path_2, embeddings)\n",
    "\n",
    "# User 1: Querying their own data\n",
    "# question_1 = \"What is the good response to the painkillers?\"\n",
    "# output_1 = user_rag_pipeline(user_1, question_1, model, embeddings)\n",
    "# print(\"User 1 Response:\", output_1)\n",
    "\n",
    "# User 2: Querying their own data\n",
    "\n",
    "pharmacogenomics_output_structure = \"\"\"{\n",
    "    \"pharmacogenomics_report\": {\n",
    "        \"<response_category_type>\": {\n",
    "            \"number_of_variants_analyzed\": \"<integer>\",\n",
    "            \"number_of_gene_markers_evaluated\": \"<integer>\",\n",
    "            \"data_validated_on\": \"<integer>\",\n",
    "            \"number_of_studies_evaluated\": \"<integer>\",\n",
    "            \"responses\": {\n",
    "                \"<if Molecule_Class else General>\": {\n",
    "                    \"Good_Response\": [\"<drug_name_1>\", \"<drug_name_2>\", \"...\"],\n",
    "                    \"Intermediate_Response\": [\"<drug_name_1>\", \"<drug_name_2>\", \"...\"],\n",
    "                    \"Poor_Response\": [\"<drug_name_1>\", \"<drug_name_2>\", \"...\"],\n",
    "                    \"Evidence_not_found\": [\"<drug_name_1>\", \"<drug_name_2>\", \"...\"]\n",
    "                    },\n",
    "                \"...\": {}\n",
    "            }\n",
    "        },\n",
    "        \"...\": {}\n",
    "    }\n",
    "    }\"\"\"\n",
    "question_2 = \"generate the structure as given below {pharmacogenomics_output_structure}\"\n",
    "output_2 = user_rag_pipeline(user_2, question_2, model, embeddings)\n",
    "print(\"User 2 Response:\", output_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
